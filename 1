# ============================================
# LAYER 0: RAW TRANSCRIPT SNAPSHOT
# Production version - æ¯æ—¥è¿è¡Œ
# ============================================

import pandas as pd
import pyodbc
import numpy as np
from datetime import datetime
import warnings
from tqdm import tqdm
from multiprocessing import Pool, cpu_count
from concurrent.futures import ProcessPoolExecutor, as_completed
from itertools import islice
import re
import html
from pathlib import Path
import torch
from sentence_transformers import SentenceTransformer
from langchain.text_splitter import RecursiveCharacterTextSplitter

warnings.filterwarnings('ignore')

# ============================================
# CONFIGURATION
# ============================================

SERVER = 'PRDRSHHISTSQL' #'PRDRSHTRIALSQL'
DATABASE = 'StreetEvents'
DRIVER = '{ODBC Driver 17 for SQL Server}'
CONN_STR = (
    f'Driver={DRIVER};'
    f'Server={SERVER};'
    f'Database={DATABASE};'
    f'Trusted_Connection=yes;'
)

#UNIVERSE_PATH = '/data09/home/aw_shared/univ_data.parquet'
UNIVERSE_PATH = '/data09/home/aw_shared/prod/univ_merged' 
OUTPUT_DIR = '/data09/home/aw_shared/prod/daily_meta_embedding'

# Embedding configuration
MODEL_NAME = "intfloat/e5-base-v2"
EMBEDDING_BATCH_SIZE = 64  # æ¯æ¬¡ encode çš„ batch size
THREADS_PER_WORKER = 8     # æ¯ä¸ªè¿›ç¨‹çš„çº¿ç¨‹æ•°

# Load universe once (global)
# df_univ = pd.read_parquet(UNIVERSE_PATH)
# df_univ['date'] = pd.to_datetime(df_univ['date'])
def load_universe(univ_dir):
    files = sorted(Path(univ_dir).glob("*.parquet"))
    dfs = [pd.read_parquet(f) for f in files]
    univ = pd.concat(dfs, ignore_index=True)
    univ['date'] = pd.to_datetime(univ['date'])
    univ = univ.sort_values('date').reset_index(drop=True)
    return univ
df_univ = load_universe(UNIVERSE_PATH)

# ============================================
# HELPER FUNCTIONS
# ============================================

def get_next_trading_day(T, df_univ):
    dates = df_univ['date'].sort_values().unique()
    T = pd.to_datetime(T).normalize()
    idx = np.searchsorted(dates, T)
    if idx + 1 < len(dates):
        return dates[idx + 1]
    else:
        return T + pd.offsets.BDay(1)

def fetch_transcript_snapshot_for_date(T):
    T = pd.to_datetime(T).normalize()
    T_next = get_next_trading_day(T, df_univ)
    print(f"Transcript snapshot for {T.date()}")
    sql_query = f"""
    SELECT
        EventId, CompanyTicker, CompanyId, Sedol, Isin, Cusip,StartDateEST, DataDate, LastUpdateEST, 
        EventTypeName, EventStoryVersion, Headline, Contents, EventTypeId, IsBackfill
    FROM
        StreetEvents..StreetEventsTranscript
    WHERE
        EventTypeId = 1
        AND DataDate >= '{T:%Y-%m-%d}'
        AND DataDate <= '{T_next:%Y-%m-%d}'
    """
    with pyodbc.connect(CONN_STR, timeout=30) as conn:
        df = pd.read_sql(sql_query, conn)
    if df.empty:
        print(f" No transcripts for {T.date()}")
        return df
   # return df
    
    df['StartDateEST'] = pd.to_datetime(df['StartDateEST'])
    df['DataDate'] = pd.to_datetime(df['DataDate'])
    df['LastUpdateEST'] = pd.to_datetime(df['LastUpdateEST'])
    return df


def map_to_universe(df_transcript, T):
    if df_transcript.empty:
        return df_transcript
    T = pd.to_datetime(T).normalize()
    T_next = get_next_trading_day(T, df_univ)
    T_plus_1_930 = T_next + pd.Timedelta(hours=9, minutes=30)
    
    #universe for date T
    df_univ_T = df_univ[df_univ['date'] == T].copy()
    if df_univ_T.empty:
        print(f" No universe data for {T.date()}")
        return pd.DataFrame()
        
    # Sedol_6
    df_univ_T['Sedol_6'] = (df_univ_T['Sedol'].astype(str).str.strip().str.upper().str[:6])
    df_transcript['Sedol_6'] = (df_transcript['Sedol'].astype(str).str.strip().str.upper().str[:6])
    
    # merge
    df_univ_T['cutoff_time'] = T_plus_1_930
    df_univ_T = df_univ_T.sort_values(['cutoff_time','Sedol_6'])
    df_transcript = df_transcript.sort_values([ 'LastUpdateEST','Sedol_6'])

    df_mapped = pd.merge_asof(df_univ_T, df_transcript, by='Sedol_6',
        left_on='cutoff_time',
        right_on='LastUpdateEST',
        direction='backward',
        allow_exact_matches=True) #for univ T, match most recent call LastUpdateEST <= cutoff_time
    
    df_mapped['date'] = T
    return df_mapped


# ============================================
# STEP 2: BUILD META.PARQUET
# ============================================


def build_meta_parquet(df_mapped: pd.DataFrame, T: str):
    """
    Build meta.parquet (ä½ åŒäº‹è¦ check çš„æ–‡ä»¶)
    
    Args:
        df_mapped: DataFrame from map_to_universe
        T: str, 'YYYY-MM-DD'
        
    Returns:
        DataFrame with meta information
    """
    if df_mapped.empty:
        return df_mapped
    
    meta_cols = [
        'date',
        'PointID',
        'Sedol_6',  # âœ… æ”¹ï¼šç”¨ Sedol_6 æ›¿ä»£ Sedol
        'EventId',
        'CompanyTicker',
        'CompanyId',
        'StartDateEST',
        'DataDate',
        'LastUpdateEST',
        'EventStoryVersion',
        'EventTypeId',
    ]
    
    meta = df_mapped[meta_cols].copy()
    
    #print(f"ğŸ“‹ Meta built: {len(meta):,} records ({meta['EventId'].notna().sum():,} with calls)")
    
    return meta

# ============================================
# STEP 3: TEXT PROCESSING
# ============================================

def process_transcript_step0(text):
    """æ¸…ç† transcript"""
    if not isinstance(text, str):
        return pd.Series(['', ''])

    split_pattern = r"={10,}\s*Presentation\s*-{10,}"
    match = re.search(split_pattern, text, re.IGNORECASE)
    
    headline_text = ""
    content_text = text 

    if match:
        split_idx = match.start()
        raw_headline = text[:split_idx]
        content_text = text[match.end():]
        
        participants_pattern = r"={10,}\s*Corporate Participants"
        part_match = re.search(participants_pattern, raw_headline, re.IGNORECASE)
        
        if part_match:
            headline_text = raw_headline[:part_match.start()].strip()
        else:
            headline_text = raw_headline.strip()
    else:
        headline_text = ""
        content_text = text            

    content_text = html.unescape(content_text)
    
    # Delete Operator chunks
    operator_pattern_with_lines = r'-{10,}\s*Operator\s*\[\d+\]\s*-{10,}(.*?)(?=-{10,}|\Z)'
    content_text = re.sub(operator_pattern_with_lines, '', content_text, flags=re.DOTALL | re.IGNORECASE)
    
    operator_pattern_no_prefix = r'^\s*Operator\s*\[\d+\]\s*-{10,}(.*?)(?=-{10,}|\Z)'
    content_text = re.sub(operator_pattern_no_prefix, '', content_text, flags=re.DOTALL | re.IGNORECASE | re.MULTILINE)

    # Drop separators
    content_text = re.sub(r'={10,}', '', content_text)
    content_text = re.sub(r'-{10,}', '', content_text)
    content_text = re.sub(r'\[\d+\]', '', content_text)
    content_text = re.sub(r'\n\s*\n+', '\n\n', content_text).strip()
    
    headline_text = html.unescape(headline_text).strip()

    return pd.Series([headline_text, content_text], index=['headline_from_content', 'contents_cleaned'])


def detect_qa_section(text):
    """æ£€æµ‹æ˜¯å¦æœ‰ Q&A section"""
    if not isinstance(text, str):
        return False
    
    qa_patterns = [
        r'Questions?\s+and\s+Answers?',
        r'Q\s*&?\s*A', 
        r'Question[s]?[\s-]and[\s-]Answer[s]?\s+Session',
        r'\n\s*Questions?\s+and\s+Answers?\s*\n',
    ]

    for pattern in qa_patterns:
        if re.search(pattern, text, re.IGNORECASE):
            return True
    
    return False


def split_presentation_qa(row):
    """åˆ†ç¦» Presentation å’Œ Q&A"""
    text = row['contents_cleaned']  
    has_qa = row['has_qa']
    
    if not has_qa or not isinstance(text, str):
        return pd.Series([text, None])
    
    qa_patterns = [
        r'Questions?\s+and\s+Answers?',
        r'Q\s*&?\s*A', 
        r'Question[s]?[\s-]and[\s-]Answer[s]?\s+Session',
        r'\n\s*Questions?\s+and\s+Answers?\s*\n',
    ]
    
    for pattern in qa_patterns:
        match = re.search(pattern, text, re.IGNORECASE)
        if match:
            split_pos = match.start()
            presentation = text[:split_pos].strip()
            qa = text[split_pos:].strip()
            return pd.Series([presentation, qa])
    
    return pd.Series([text, None])


SPEAKER_PATTERN = re.compile(
    r'^[A-Z][a-zA-Z]+(?:\s[A-Z][a-zA-Z]+)*,\s+.*-\s+.*$')

def merge_speaker_blocks(text: str) -> list:
    """åˆå¹¶ speaker blocks"""
    blocks = []
    current_lines = []

    for line in text.splitlines():
        line = line.strip()
        if not line:
            continue

        if SPEAKER_PATTERN.match(line):
            if current_lines:
                blocks.append(" ".join(current_lines))
                current_lines = []
        else:
            current_lines.append(line)

    if current_lines:
        blocks.append(" ".join(current_lines))

    return blocks


# ============================================
# STEP 4: CHUNKING (ä½¿ç”¨ä½ çš„å¹¶è¡Œé€»è¾‘)
# ============================================

text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=1024,
    chunk_overlap=128,
    length_function=len,
    separators=["\n\n", ". ", "\n", " ", ""]
)


def process_batch_chunking(batch):
    """å¤„ç†ä¸€ä¸ª batch çš„ chunkingï¼ˆç”¨äºå¹¶è¡Œï¼‰"""
    out = []
    for idx, row in batch:
        text = row['Presentation']
        if pd.isna(text) or not text.strip():
            continue

        # Merge speaker blocks
        blocks = merge_speaker_blocks(text)

        chunks = []
        for block in blocks:
            chunks.extend(text_splitter.split_text(block))

        out.extend([
            {
                "original_index": idx,
                "chunk_seq_id": i,
                "chunk_text": chunk,
                "EventId": row['EventId'],
               # "Sedol": row['Sedol'],
                "Sedol_6": row['Sedol_6'],
                "StartDateEST": row['StartDateEST'],
                "LastUpdateEST": row['LastUpdateEST'],
                "DataDate": row['DataDate']
            }
            for i, chunk in enumerate(chunks)
        ])

    return out


def batch_iterator(iterable, batch_size):
    """ç”Ÿæˆ batch iterator"""
    it = iter(iterable)
    while True:
        batch = list(islice(it, batch_size))
        if not batch:
            break
        yield batch


def chunk_presentations(df_mapped: pd.DataFrame):
    """
    Chunk presentationsï¼ˆä½¿ç”¨ä½ çš„å¹¶è¡Œé€»è¾‘ï¼‰
    
    å¯¹äº daily productionï¼š
    - å¦‚æœ calls < 100: ä¸å¹¶è¡Œï¼ˆoverhead å¤§äºæ”¶ç›Šï¼‰
    - å¦‚æœ calls >= 100: ä½¿ç”¨å¹¶è¡ŒåŠ é€Ÿ
    """
    # åªå¤„ç†æœ‰ call çš„è®°å½•
    df_with_calls = df_mapped[df_mapped['EventId'].notna()].copy()
    
    if df_with_calls.empty:
        print(" No calls to chunk")
        return pd.DataFrame()
    
   # print(f"\nğŸ“ Chunking {len(df_with_calls):,} presentations...")
    
    # å†³å®šæ˜¯å¦å¹¶è¡Œ
    USE_PARALLEL = len(df_with_calls) >= 100
    
    if USE_PARALLEL:
        # å¹¶è¡Œå¤„ç†ï¼ˆä½ çš„åŸå§‹é€»è¾‘ï¼‰
        indexed_data = list(df_with_calls.iterrows())
        BATCH_SIZE = 32
        batches = list(batch_iterator(indexed_data, BATCH_SIZE))
        
        n_workers = min(16, max(1, cpu_count() - 2))  # Daily ä¸éœ€è¦å¤ªå¤š workers
        #print(f"   Using {n_workers} workers, batch_size={BATCH_SIZE}")
        
        all_chunks = []
        with Pool(n_workers) as pool:
            for batch_result in tqdm(
                pool.imap(process_batch_chunking, batches, chunksize=1),
                total=len(batches),
                desc="Chunking"
            ):
                all_chunks.extend(batch_result)
        
        df_chunks = pd.DataFrame(all_chunks)
    
    else:
        # ä¸²è¡Œå¤„ç†ï¼ˆå°æ•°æ®é‡æ›´å¿«ï¼‰
       # print(f"   Using serial processing (small dataset)")
        all_chunks = []
        
        for idx, row in tqdm(df_with_calls.iterrows(), total=len(df_with_calls), desc="Chunking"):
            text = row['Presentation']
            if pd.isna(text) or not text.strip():
                continue
            
            blocks = merge_speaker_blocks(text)
            chunks = []
            for block in blocks:
                chunks.extend(text_splitter.split_text(block))
            
            for i, chunk in enumerate(chunks):
                all_chunks.append({
                    'EventId': row['EventId'],
                    #'Sedol': row['Sedol'],
                    'Sedol_6': row['Sedol_6'],
                    'StartDateEST': row['StartDateEST'],
                    'LastUpdateEST': row['LastUpdateEST'],
                    'DataDate': row['DataDate'],
                    'chunk_seq_id': i,
                    'chunk_text': chunk
                })
        
        df_chunks = pd.DataFrame(all_chunks)
    
    #print(f"âœ… Created {len(df_chunks):,} chunks from {df_chunks['EventId'].nunique():,} events")
    # if len(df_chunks) > 0:
    #     print(f"   Avg chunks per event: {len(df_chunks)/df_chunks['EventId'].nunique():.1f}")
    
    return df_chunks


# ============================================
# STEP 5: GENERATE EMBEDDINGS (ç®€åŒ–ç‰ˆï¼Œæ—  checkpoint)
# ============================================

def process_embedding_batch(args):
    """
    å¤„ç†ä¸€ä¸ª batch çš„ embeddingï¼ˆç”¨äºå¹¶è¡Œï¼‰
    
    Args:
        args: tuple of (batch_id, batch_data)
              batch_data æ˜¯ DataFrameï¼ŒåŒ…å« texts å’Œå…¶ä»–åˆ—
    """
    batch_id, batch_df = args  # âœ… æ”¹ï¼šæ¥æ”¶ DataFrame è€Œä¸æ˜¯ texts
    
    # è®¾ç½®çº¿ç¨‹æ•°
    torch.set_num_threads(THREADS_PER_WORKER)
    
    # åŠ è½½æ¨¡å‹ï¼ˆæ¯ä¸ªè¿›ç¨‹ç‹¬ç«‹åŠ è½½ï¼‰
    model = SentenceTransformer(MODEL_NAME, device='cpu')
    
    # æ·»åŠ  "passage: " prefix
    texts_with_prefix = ["passage: " + str(t) for t in batch_df['chunk_text'].tolist()]
    
    # ç”Ÿæˆ embeddings
    embeddings = model.encode(
        texts_with_prefix,
        batch_size=EMBEDDING_BATCH_SIZE,
        show_progress_bar=False,
        convert_to_numpy=True,
        normalize_embeddings=True
    )
    
    # âœ… æ”¹ï¼šè¿”å› DataFrame è€Œä¸æ˜¯åªè¿”å› embeddings
    batch_df['embedding'] = [emb.tolist() for emb in embeddings]
    
    return batch_id, batch_df

def generate_embeddings(df_chunks: pd.DataFrame):
    """ç”Ÿæˆ embeddings"""
    if df_chunks.empty:
        print(" No chunks to embed")
        return pd.DataFrame(columns=['EventId', 'Sedol_6', 'StartDateEST', 'LastUpdateEST', 'chunk_seq_id', 'embedding'])
    
    #print(f"\nğŸ”„ Generating embeddings for {len(df_chunks):,} chunks...")
    
    USE_PARALLEL = len(df_chunks) >= 1000
    
    if USE_PARALLEL:
        # å¹¶è¡Œå¤„ç†
        BATCH_SIZE = 500
        n_batches = (len(df_chunks) + BATCH_SIZE - 1) // BATCH_SIZE
        n_workers = min(8, max(1, cpu_count() // 2))
        
       # print(f"   Using {n_workers} workers, {n_batches} batches")
        
        # âœ… æ”¹ï¼šå‡†å¤‡ batches - ä¼ é€’ DataFrame è€Œä¸æ˜¯ texts
        tasks = []
        for i in range(n_batches):
            start_idx = i * BATCH_SIZE
            end_idx = min((i + 1) * BATCH_SIZE, len(df_chunks))
            batch_df = df_chunks.iloc[start_idx:end_idx].copy()  # âœ… ä¼ é€’æ•´ä¸ª DataFrame
            tasks.append((i, batch_df))
        
        # å¹¶è¡Œå¤„ç†
        results = {}
        with ProcessPoolExecutor(max_workers=n_workers) as executor:
            futures = {executor.submit(process_embedding_batch, task): task[0] for task in tasks}
            
            for future in tqdm(as_completed(futures), total=len(futures), desc="Embedding"):
                batch_id, batch_df = future.result()  # âœ… æ”¹ï¼šæ¥æ”¶ DataFrame
                results[batch_id] = batch_df
        
        # âœ… æ”¹ï¼šåˆå¹¶ç»“æœ - concat DataFrames
        all_batches = [results[i] for i in range(n_batches)]
        df_embeddings = pd.concat(all_batches, ignore_index=True)
    
    else:
        # ä¸²è¡Œå¤„ç†
       # print(f"   Using serial processing (small dataset)")
        
        model = SentenceTransformer(MODEL_NAME, device='cpu')
        texts = ["passage: " + str(t) for t in df_chunks['chunk_text'].tolist()]
        
        embeddings = model.encode(
            texts,
            batch_size=EMBEDDING_BATCH_SIZE,
            show_progress_bar=True,
            convert_to_numpy=True,
            normalize_embeddings=True
        )
        
        df_chunks['embedding'] = [emb.tolist() for emb in embeddings]
        df_embeddings = df_chunks.copy()  # âœ… æ”¹ï¼šä¿ç•™æ‰€æœ‰åˆ—
    
    # âœ… æ”¹ï¼šé€‰æ‹©éœ€è¦çš„åˆ—
    df_embeddings = df_embeddings[['EventId', 'Sedol_6', 'StartDateEST', 'LastUpdateEST', 'chunk_seq_id', 'embedding']].copy()
    
  #  print(f"âœ… Generated {len(df_embeddings):,} embeddings")
   # print(f"   Embedding dimension: {len(df_embeddings['embedding'].iloc[0])}")
    
    return df_embeddings

# ============================================
# STEP 6: SAVE OUTPUTS
# ============================================

def save_outputs(meta: pd.DataFrame, embeddings: pd.DataFrame, T: str):
    """Save meta.parquet and embeddings.parquet"""
    import os
    
    # âœ… æ”¹ï¼šåˆ›å»ºå­ç›®å½•
    meta_dir = f"{OUTPUT_DIR}/meta"
    emb_dir = f"{OUTPUT_DIR}/embeddings"
    os.makedirs(meta_dir, exist_ok=True)
    os.makedirs(emb_dir, exist_ok=True)
    
    date_str = pd.to_datetime(T).strftime('%Y%m%d')
    
    # âœ… æ”¹ï¼šä¿å­˜åˆ°å­ç›®å½•ï¼Œæ–‡ä»¶ååªç”¨æ—¥æœŸ
    meta_path = f"{meta_dir}/{date_str}.parquet"
    meta.to_parquet(meta_path, index=False)
   # print(f"\nğŸ’¾ Saved meta: {meta_path}")
    #print(f"   Size: {len(meta):,} records")
    
    # Save embeddings
    if not embeddings.empty:
        emb_path = f"{emb_dir}/{date_str}.parquet"
        embeddings.to_parquet(emb_path, index=False)
      #  print(f"ğŸ’¾ Saved embeddings: {emb_path}")
      #  print(f"   Size: {len(embeddings):,} chunks")
    else:
        emb_path = None
       # print(f"â„¹ï¸ No embeddings to save")
    
    return meta_path, emb_path

# ============================================
# MAIN PIPELINE
# ============================================

def run_daily_snapshot(T: str):
    """
    ä¸»å‡½æ•°ï¼šè¿è¡Œå®Œæ•´çš„ daily snapshot pipeline
    
    Args:
        T: str, 'YYYY-MM-DD' - trading date (EOD)
        
    Returns:
        tuple: (meta_path, embeddings_path)
    """
    # print(f"\n{'#'*80}")
    # print(f"# LAYER 0 SNAPSHOT PIPELINE")
   # print(f"Date: {T}")
    #print(f"{'-'*80}")
    
    # Step 1: Fetch raw transcripts
    df_transcript = fetch_transcript_snapshot_for_date(T)
    
    # Step 2: Map to universe
    df_mapped = map_to_universe(df_transcript, T)
    if df_mapped.empty:
        print(f"\nâš ï¸ No universe data for {T}, skipping...")
        return None, None
    
    # Step 3: Build meta
    meta = build_meta_parquet(df_mapped, T)
    
    # å¦‚æœæ²¡æœ‰ä»»ä½• callsï¼Œç›´æ¥ä¿å­˜ meta å¹¶è¿”å›
    if df_mapped['EventId'].notna().sum() == 0:
        #print(f"\nâ„¹ï¸ No calls for {T}, saving meta only...")
        meta_path = save_outputs(meta, pd.DataFrame(), T)[0]
        return meta_path, None
    
    # Step 4: Clean transcripts (åªå¤„ç†æœ‰ call çš„)
   # print(f"\nğŸ§¹ Cleaning transcripts...")
    df_with_calls = df_mapped[df_mapped['Contents'].notna()].copy()
    df_result = df_with_calls['Contents'].apply(process_transcript_step0)
    df_with_calls = pd.concat([df_with_calls, df_result], axis=1)
    
    # Step 5: Detect QA
   # print(f"ğŸ” Detecting Q&A sections...")
    df_with_calls['has_qa'] = df_with_calls['contents_cleaned'].apply(detect_qa_section)
  #  print(f"   Has Q&A: {df_with_calls['has_qa'].sum()} / {len(df_with_calls)}")
    
    # Step 6: Split presentation and QA
  #  print(f"âœ‚ï¸ Splitting presentation and Q&A...")
    result = df_with_calls.apply(split_presentation_qa, axis=1)
    df_with_calls[['Presentation', 'QA']] = result
    
    # æ›´æ–° df_mapped
    df_mapped.loc[df_with_calls.index, ['contents_cleaned', 'has_qa', 'Presentation', 'QA']] = \
        df_with_calls[['contents_cleaned', 'has_qa', 'Presentation', 'QA']]
    
    # Step 7: Chunk presentations
    df_chunks = chunk_presentations(df_mapped)
    
    # Step 8: Generate embeddings
    if not df_chunks.empty:
        embeddings = generate_embeddings(df_chunks)
    else:
        embeddings = pd.DataFrame()
    
    # Step 9: Save outputs
    meta_path, emb_path = save_outputs(meta, embeddings, T)
    
    # print(f"\n{'='*80}")
    # print(f"âœ… PIPELINE COMPLETED FOR {T}")
    # print(f"{'='*80}")
    # print(f"Meta records: {len(meta):,}")
    # print(f"Embedding chunks: {len(embeddings):,}")
    
    return meta_path, emb_path


# ============================================
# BATCH PROCESSING
# ============================================

def run_batch_snapshots(start_date, end_date):
    """æ‰¹é‡è¿è¡Œå¤šä¸ªæ—¥æœŸ"""
    # ä½¿ç”¨ universe çš„å®é™…äº¤æ˜“æ—¥
    start_date = pd.to_datetime(start_date)
    end_date = pd.to_datetime(end_date)
    date_range = df_univ[(df_univ['date'] >= start_date) & 
                         (df_univ['date'] <= end_date)]['date'].unique()
    date_range = sorted(date_range)
    
    print(f"\n{'='*80}")
    print(f"BATCH PROCESSING")
    print(f"Date range: {start_date} to {end_date}")
    print(f"{'='*80}")
    
    results = []
    
    for date in date_range:
        date_str = pd.to_datetime(date).strftime('%Y-%m-%d')
        
        try:
            meta_path, emb_path = run_daily_snapshot(date_str)
            results.append({
                'date': date_str,
                'status': 'success' if meta_path else 'no_data',
                'meta_path': meta_path,
                'emb_path': emb_path
            })
        except Exception as e:
            print(f"\n Failed for {date_str}: {str(e)}")
            import traceback
            traceback.print_exc()
            results.append({
                'date': date_str,
                'status': 'failed',
                'error': str(e)
            })
            continue
    
    # Summary
    df_results = pd.DataFrame(results)
    # print(f"\n{'='*80}")
    # print(f"BATCH PROCESSING SUMMARY")
    # print(f"{'='*80}")
    # print(df_results['status'].value_counts())
    
    # Save results
    # results_path = f"{OUTPUT_DIR}/batch_results.csv"
    # df_results.to_csv(results_path, index=False)
   # print(f"\nğŸ’¾ Batch results saved to: {results_path}")
    
    return df_results


# ============================================
# USAGE
# ============================================

if __name__ == "__main__":
    
#    # Single date
#     T = '2024-01-17'
#     meta_path, emb_path = run_daily_snapshot(T)
    
  #  Batch processing
    # results = run_batch_snapshots(
    #     #start_date='2015-01-01',
    #     start_date='2025-12-29',
    #     end_date='2026-02-06'
    # )
    results = run_batch_snapshots(
        #start_date='2015-01-01',
        # start_date='2025-12-27',
        # end_date='2025-12-30'
        start_date='2026-02-10',
        end_date='2026-02-10'
    )
