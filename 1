ä¸–ç•Œæ—¶é’Ÿ:: ç¾å›½- å½“å‰æ—¶é—´

24 Time Zones
https://24timezones.com â€º ç¾å›½ â€º æ—¶é—´
Â·
è½‰ç‚ºç¹é«”ç¶²é 
ç¾å›½:: å½“å‰æ—¶é—´ Â· èŠåŠ å“¥ Â· ä¼‘æ–¯æ•¦ Â· åœ£å®‰ä¸œå°¼å¥¥ Â· é”æ‹‰æ–¯. 1:39 AM. æ˜ŸæœŸå››, äºŒæœˆ12, 2026. GMT -06:00. MST Â· ä¸¹ä½› Â· åŸƒå°”å¸• ...
ç¾å›½ç°åœ¨å‡ ç‚¹
æ—¶é—´åœ°å›¾ç½‘
http://m.24timemap.com â€º usa
Â·
è½‰ç‚ºç¹é«”ç¶²é 
ç¾å›½æ—¶åŒºä¿¡æ¯ Â· å½“å‰æ—¶é—´ï¼š. 2026å¹´02æœˆ12æ—¥10:16:04 æ˜ŸæœŸå›› (é¦–éƒ½:åç››é¡¿) Â· æ—¶åŒºè·¨è¶Šï¼š. è¥¿ååŒºè¥¿äº”åŒºè¥¿å…­åŒºè¥¿ä¸ƒåŒºè¥¿å…«åŒºè¥¿ä¹åŒº Â· å›½å®¶åŒºå·ï¼š. +1 Â· è¯­è¨€ï¼š. è‹±è¯­ Â· è´§å¸ï¼š.
Time.gov

time.gov
https://www.time.gov
Â·
ç¿»è­¯é€™å€‹ç¶²é 
10:08:12 P.M.. Arizona Mountain Standard Time. MST (UTC-7).
åœ–ç‰‡
ç¾å›½æ—¶é—´- å…¨çƒæ—¶åŒºæŸ¥è¯¢- ä¸–ç•Œæ—¶é—´å’Œæ—¶å·®æŸ¥è¯¢
ç¾å›½æ—¶é—´- å…¨çƒæ—¶åŒºæŸ¥è¯¢- ä¸–ç•Œæ—¶é—´å’Œæ—¶å·®æŸ¥è¯¢

www.artjoey.com

# ============================================
# LAYER 0: RAW TRANSCRIPT SNAPSHOT
# Production version - æ¯æ—¥è¿è¡Œ
# ============================================

import pandas as pd
import pyodbc
import numpy as np
from datetime import datetime
import warnings
from tqdm import tqdm
from multiprocessing import Pool, cpu_count
from concurrent.futures import ProcessPoolExecutor, as_completed
from itertools import islice
import re
import html
from pathlib import Path
import torch
from sentence_transformers import SentenceTransformer
from langchain.text_splitter import RecursiveCharacterTextSplitter

warnings.filterwarnings('ignore')

# ============================================
# CONFIGURATION
# ============================================

SERVER = 'PRDRSHHISTSQL' #'PRDRSHTRIALSQL'
DATABASE = 'StreetEvents'
DRIVER = '{ODBC Driver 17 for SQL Server}'
CONN_STR = (
    f'Driver={DRIVER};'
    f'Server={SERVER};'
    f'Database={DATABASE};'
    f'Trusted_Connection=yes;'
)

#UNIVERSE_PATH = '/data09/home/aw_shared/univ_data.parquet'
UNIVERSE_PATH = '/data09/home/aw_shared/prod/univ_merged' 
OUTPUT_DIR = '/data09/home/aw_shared/prod/daily_meta_embedding'

# Embedding configuration
MODEL_NAME = "intfloat/e5-base-v2"
EMBEDDING_BATCH_SIZE = 64  # æ¯æ¬¡ encode çš„ batch size
THREADS_PER_WORKER = 8     # æ¯ä¸ªè¿›ç¨‹çš„çº¿ç¨‹æ•°

# Load universe once (global)
# df_univ = pd.read_parquet(UNIVERSE_PATH)
# df_univ['date'] = pd.to_datetime(df_univ['date'])
def load_universe(univ_dir):
    files = sorted(Path(univ_dir).glob("*.parquet"))
    dfs = [pd.read_parquet(f) for f in files]
    univ = pd.concat(dfs, ignore_index=True)
    univ['date'] = pd.to_datetime(univ['date'])
    univ = univ.sort_values('date').reset_index(drop=True)
    return univ
df_univ = load_universe(UNIVERSE_PATH)

# ============================================
# HELPER FUNCTIONS
# ============================================

def get_next_trading_day(T, df_univ):
    dates = df_univ['date'].sort_values().unique()
    T = pd.to_datetime(T).normalize()
    idx = np.searchsorted(dates, T)
    if idx + 1 < len(dates):
        return dates[idx + 1]
    else:
        return T + pd.offsets.BDay(1)

def fetch_transcript_snapshot_for_date(T):
    T = pd.to_datetime(T).normalize()
    T_next = get_next_trading_day(T, df_univ)
    print(f"Transcript snapshot for {T.date()}")
    sql_query = f"""
    SELECT
        EventId, CompanyTicker, CompanyId, Sedol, Isin, Cusip,StartDateEST, DataDate, LastUpdateEST, 
        EventTypeName, EventStoryVersion, Headline, Contents, EventTypeId, IsBackfill
    FROM
        StreetEvents..StreetEventsTranscript
    WHERE
        EventTypeId = 1
        AND DataDate >= '{T:%Y-%m-%d}'
        AND DataDate <= '{T_next:%Y-%m-%d}'
    """
    with pyodbc.connect(CONN_STR, timeout=30) as conn:
        df = pd.read_sql(sql_query, conn)
    if df.empty:
        print(f" No transcripts for {T.date()}")
        return df
   # return df
    
    df['StartDateEST'] = pd.to_datetime(df['StartDateEST'])
    df['DataDate'] = pd.to_datetime(df['DataDate'])
    df['LastUpdateEST'] = pd.to_datetime(df['LastUpdateEST'])
    return df


def map_to_universe(df_transcript, T):
    if df_transcript.empty:
        return df_transcript
    T = pd.to_datetime(T).normalize()
    T_next = get_next_trading_day(T, df_univ)
    T_plus_1_930 = T_next + pd.Timedelta(hours=9, minutes=30)
    
    #universe for date T
    df_univ_T = df_univ[df_univ['date'] == T].copy()
    if df_univ_T.empty:
        print(f" No universe data for {T.date()}")
        return pd.DataFrame()
        
    # Sedol_6
    df_univ_T['Sedol_6'] = (df_univ_T['Sedol'].astype(str).str.strip().str.upper().str[:6])
    df_transcript['Sedol_6'] = (df_transcript['Sedol'].astype(str).str.strip().str.upper().str[:6])
    
    # merge
    df_univ_T['cutoff_time'] = T_plus_1_930
    df_univ_T = df_univ_T.sort_values(['cutoff_time','Sedol_6'])
    df_transcript = df_transcript.sort_values([ 'LastUpdateEST','Sedol_6'])

    df_mapped = pd.merge_asof(df_univ_T, df_transcript, by='Sedol_6',
        left_on='cutoff_time',
        right_on='LastUpdateEST',
        direction='backward',
        allow_exact_matches=True) #for univ T, match most recent call LastUpdateEST <= cutoff_time
    
    df_mapped['date'] = T
    return df_mapped


# ============================================
# STEP 2: BUILD META.PARQUET
# ============================================


def build_meta_parquet(df_mapped: pd.DataFrame, T: str):
    """
    Build meta.parquet (ä½ åŒäº‹è¦ check çš„æ–‡ä»¶)
    
    Args:
        df_mapped: DataFrame from map_to_universe
        T: str, 'YYYY-MM-DD'
        
    Returns:
        DataFrame with meta information
    """
    if df_mapped.empty:
        return df_mapped
    
    meta_cols = [
        'date',
        'PointID',
        'Sedol_6',  # âœ… æ”¹ï¼šç”¨ Sedol_6 æ›¿ä»£ Sedol
        'EventId',
        'CompanyTicker',
        'CompanyId',
        'StartDateEST',
        'DataDate',
        'LastUpdateEST',
        'EventStoryVersion',
        'EventTypeId',
    ]
    
    meta = df_mapped[meta_cols].copy()
    
    #print(f"ğŸ“‹ Meta built: {len(meta):,} records ({meta['EventId'].notna().sum():,} with calls)")
    
    return meta

# ============================================
# STEP 3: TEXT PROCESSING
# ============================================

def process_transcript_step0(text):
    """æ¸…ç† transcript"""
    if not isinstance(text, str):
        return pd.Series(['', ''])

    split_pattern = r"={10,}\s*Presentation\s*-{10,}"
    match = re.search(split_pattern, text, re.IGNORECASE)
    
    headline_text = ""
    content_text = text 

    if match:
        split_idx = match.start()
        raw_headline = text[:split_idx]
        content_text = text[match.end():]
        
        participants_pattern = r"={10,}\s*Corporate Participants"
        part_match = re.search(participants_pattern, raw_headline, re.IGNORECASE)
        
        if part_match:
            headline_text = raw_headline[:part_match.start()].strip()
        else:
            headline_text = raw_headline.strip()
    else:
        headline_text = ""
        content_text = text            

    content_text = html.unescape(content_text)
    
    # Delete Operator chunks
    operator_pattern_with_lines = r'-{10,}\s*Operator\s*\[\d+\]\s*-{10,}(.*?)(?=-{10,}|\Z)'
    content_text = re.sub(operator_pattern_with_lines, '', content_text, flags=re.DOTALL | re.IGNORECASE)
    
    operator_pattern_no_prefix = r'^\s*Operator\s*\[\d+\]\s*-{10,}(.*?)(?=-{10,}|\Z)'
    content_text = re.sub(operator_pattern_no_prefix, '', content_text, flags=re.DOTALL | re.IGNORECASE | re.MULTILINE)

    # Drop separators
    content_text = re.sub(r'={10,}', '', content_text)
    content_text = re.sub(r'-{10,}', '', content_text)
    content_text = re.sub(r'\[\d+\]', '', content_text)
    content_text = re.sub(r'\n\s*\n+', '\n\n', content_text).strip()
    
    headline_text = html.unescape(headline_text).strip()

    return pd.Series([headline_text, content_text], index=['headline_from_content', 'contents_cleaned'])


def detect_qa_section(text):
    """æ£€æµ‹æ˜¯å¦æœ‰ Q&A section"""
    if not isinstance(text, str):
        return False
    
    qa_patterns = [
        r'Questions?\s+and\s+Answers?',
        r'Q\s*&?\s*A', 
        r'Question[s]?[\s-]and[\s-]Answer[s]?\s+Session',
        r'\n\s*Questions?\s+and\s+Answers?\s*\n',
    ]

    for pattern in qa_patterns:
        if re.search(pattern, text, re.IGNORECASE):
            return True
    
    return False


def split_presentation_qa(row):
    """åˆ†ç¦» Presentation å’Œ Q&A"""
    text = row['contents_cleaned']  
    has_qa = row['has_qa']
    
    if not has_qa or not isinstance(text, str):
        return pd.Series([text, None])
    
    qa_patterns = [
        r'Questions?\s+and\s+Answers?',
        r'Q\s*&?\s*A', 
        r'Question[s]?[\s-]and[\s-]Answer[s]?\s+Session',
        r'\n\s*Questions?\s+and\s+Answers?\s*\n',
    ]
    
    for pattern in qa_patterns:
        match = re.search(pattern, text, re.IGNORECASE)
        if match:
            split_pos = match.start()
            presentation = text[:split_pos].strip()
            qa = text[split_pos:].strip()
            return pd.Series([presentation, qa])
    
    return pd.Series([text, None])


SPEAKER_PATTERN = re.compile(
    r'^[A-Z][a-zA-Z]+(?:\s[A-Z][a-zA-Z]+)*,\s+.*-\s+.*$')

def merge_speaker_blocks(text: str) -> list:
    """åˆå¹¶ speaker blocks"""
    blocks = []
    current_lines = []

    for line in text.splitlines():
        line = line.strip()
        if not line:
            continue

        if SPEAKER_PATTERN.match(line):
            if current_lines:
                blocks.append(" ".join(current_lines))
                current_lines = []
        else:
            current_lines.append(line)

    if current_lines:
        blocks.append(" ".join(current_lines))

    return blocks


# ============================================
# STEP 4: CHUNKING (ä½¿ç”¨ä½ çš„å¹¶è¡Œé€»è¾‘)
# ============================================

text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=1024,
    chunk_overlap=128,
    length_function=len,
    separators=["\n\n", ". ", "\n", " ", ""]
)


def process_batch_chunking(batch):
    """å¤„ç†ä¸€ä¸ª batch çš„ chunkingï¼ˆç”¨äºå¹¶è¡Œï¼‰"""
    out = []
    for idx, row in batch:
        text = row['Presentation']
        if pd.isna(text) or not text.strip():
            continue

        # Merge speaker blocks
        blocks = merge_speaker_blocks(text)

        chunks = []
        for block in blocks:
            chunks.extend(text_splitter.split_text(block))

        out.extend([
            {
                "original_index": idx,
                "chunk_seq_id": i,
                "chunk_text": chunk,
                "EventId": row['EventId'],
               # "Sedol": row['Sedol'],
                "Sedol_6": row['Sedol_6'],
                "StartDateEST": row['StartDateEST'],
                "LastUpdateEST": row['LastUpdateEST'],
                "DataDate": row['DataDate']
            }
            for i, chunk in enumerate(chunks)
        ])

    return out


def batch_iterator(iterable, batch_size):
    """ç”Ÿæˆ batch iterator"""
    it = iter(iterable)
    while True:
        batch = list(islice(it, batch_size))
        if not batch:
            break
        yield batch


def chunk_presentations(df_mapped: pd.DataFrame):
    """
    Chunk presentationsï¼ˆä½¿ç”¨ä½ çš„å¹¶è¡Œé€»è¾‘ï¼‰
    
    å¯¹äº daily productionï¼š
    - å¦‚æœ calls < 100: ä¸å¹¶è¡Œï¼ˆoverhead å¤§äºæ”¶ç›Šï¼‰
    - å¦‚æœ calls >= 100: ä½¿ç”¨å¹¶è¡ŒåŠ é€Ÿ
    """
    # åªå¤„ç†æœ‰ call çš„è®°å½•
    df_with_calls = df_mapped[df_mapped['EventId'].notna()].copy()
    
    if df_with_calls.empty:
        print(" No calls to chunk")
        return pd.DataFrame()
    
   # print(f"\nğŸ“ Chunking {len(df_with_calls):,} presentations...")
    
    # å†³å®šæ˜¯å¦å¹¶è¡Œ
    USE_PARALLEL = len(df_with_calls) >= 100
    
    if USE_PARALLEL:
        # å¹¶è¡Œå¤„ç†ï¼ˆä½ çš„åŸå§‹é€»è¾‘ï¼‰
        indexed_data = list(df_with_calls.iterrows())
        BATCH_SIZE = 32
        batches = list(batch_iterator(indexed_data, BATCH_SIZE))
        
        n_workers = min(16, max(1, cpu_count() - 2))  # Daily ä¸éœ€è¦å¤ªå¤š workers
        #print(f"   Using {n_workers} workers, batch_size={BATCH_SIZE}")
        
        all_chunks = []
        with Pool(n_workers) as pool:
            for batch_result in tqdm(
                pool.imap(process_batch_chunking, batches, chunksize=1),
                total=len(batches),
                desc="Chunking"
            ):
                all_chunks.extend(batch_result)
        
        df_chunks = pd.DataFrame(all_chunks)
    
    else:
        # ä¸²è¡Œå¤„ç†ï¼ˆå°æ•°æ®é‡æ›´å¿«ï¼‰
       # print(f"   Using serial processing (small dataset)")
        all_chunks = []
        
        for idx, row in tqdm(df_with_calls.iterrows(), total=len(df_with_calls), desc="Chunking"):
            text = row['Presentation']
            if pd.isna(text) or not text.strip():
                continue
            
            blocks = merge_speaker_blocks(text)
            chunks = []
            for block in blocks:
                chunks.extend(text_splitter.split_text(block))
            
            for i, chunk in enumerate(chunks):
                all_chunks.append({
                    'EventId': row['EventId'],
                    #'Sedol': row['Sedol'],
                    'Sedol_6': row['Sedol_6'],
                    'StartDateEST': row['StartDateEST'],
                    'LastUpdateEST': row['LastUpdateEST'],
                    'DataDate': row['DataDate'],
                    'chunk_seq_id': i,
                    'chunk_text': chunk
                })
        
        df_chunks = pd.DataFrame(all_chunks)
    
    #print(f"âœ… Created {len(df_chunks):,} chunks from {df_chunks['EventId'].nunique():,} events")
    # if len(df_chunks) > 0:
    #     print(f"   Avg chunks per event: {len(df_chunks)/df_chunks['EventId'].nunique():.1f}")
    
    return df_chunks


# ============================================
# STEP 5: GENERATE EMBEDDINGS (ç®€åŒ–ç‰ˆï¼Œæ—  checkpoint)
# ============================================

def process_embedding_batch(args):
    """
    å¤„ç†ä¸€ä¸ª batch çš„ embeddingï¼ˆç”¨äºå¹¶è¡Œï¼‰
    
    Args:
        args: tuple of (batch_id, batch_data)
              batch_data æ˜¯ DataFrameï¼ŒåŒ…å« texts å’Œå…¶ä»–åˆ—
    """
    batch_id, batch_df = args  # âœ… æ”¹ï¼šæ¥æ”¶ DataFrame è€Œä¸æ˜¯ texts
    
    # è®¾ç½®çº¿ç¨‹æ•°
    torch.set_num_threads(THREADS_PER_WORKER)
    
    # åŠ è½½æ¨¡å‹ï¼ˆæ¯ä¸ªè¿›ç¨‹ç‹¬ç«‹åŠ è½½ï¼‰
    model = SentenceTransformer(MODEL_NAME, device='cpu')
    
    # æ·»åŠ  "passage: " prefix
    texts_with_prefix = ["passage: " + str(t) for t in batch_df['chunk_text'].tolist()]
    
    # ç”Ÿæˆ embeddings
    embeddings = model.encode(
        texts_with_prefix,
        batch_size=EMBEDDING_BATCH_SIZE,
        show_progress_bar=False,
        convert_to_numpy=True,
        normalize_embeddings=True
    )
    
    # âœ… æ”¹ï¼šè¿”å› DataFrame è€Œä¸æ˜¯åªè¿”å› embeddings
    batch_df['embedding'] = [emb.tolist() for emb in embeddings]
    
    return batch_id, batch_df

def generate_embeddings(df_chunks: pd.DataFrame):
    """ç”Ÿæˆ embeddings"""
    if df_chunks.empty:
        print(" No chunks to embed")
        return pd.DataFrame(columns=['EventId', 'Sedol_6', 'StartDateEST', 'LastUpdateEST', 'chunk_seq_id', 'embedding'])
    
    #print(f"\nğŸ”„ Generating embeddings for {len(df_chunks):,} chunks...")
    
    USE_PARALLEL = len(df_chunks) >= 1000
    
    if USE_PARALLEL:
        # å¹¶è¡Œå¤„ç†
        BATCH_SIZE = 500
        n_batches = (len(df_chunks) + BATCH_SIZE - 1) // BATCH_SIZE
        n_workers = min(8, max(1, cpu_count() // 2))
        
       # print(f"   Using {n_workers} workers, {n_batches} batches")
        
        # âœ… æ”¹ï¼šå‡†å¤‡ batches - ä¼ é€’ DataFrame è€Œä¸æ˜¯ texts
        tasks = []
        for i in range(n_batches):
            start_idx = i * BATCH_SIZE
            end_idx = min((i + 1) * BATCH_SIZE, len(df_chunks))
            batch_df = df_chunks.iloc[start_idx:end_idx].copy()  # âœ… ä¼ é€’æ•´ä¸ª DataFrame
            tasks.append((i, batch_df))
        
        # å¹¶è¡Œå¤„ç†
        results = {}
        with ProcessPoolExecutor(max_workers=n_workers) as executor:
            futures = {executor.submit(process_embedding_batch, task): task[0] for task in tasks}
            
            for future in tqdm(as_completed(futures), total=len(futures), desc="Embedding"):
                batch_id, batch_df = future.result()  # âœ… æ”¹ï¼šæ¥æ”¶ DataFrame
                results[batch_id] = batch_df
        
        # âœ… æ”¹ï¼šåˆå¹¶ç»“æœ - concat DataFrames
        all_batches = [results[i] for i in range(n_batches)]
        df_embeddings = pd.concat(all_batches, ignore_index=True)
    
    else:
        # ä¸²è¡Œå¤„ç†
       # print(f"   Using serial processing (small dataset)")
        
        model = SentenceTransformer(MODEL_NAME, device='cpu')
        texts = ["passage: " + str(t) for t in df_chunks['chunk_text'].tolist()]
        
        embeddings = model.encode(
            texts,
            batch_size=EMBEDDING_BATCH_SIZE,
            show_progress_bar=True,
            convert_to_numpy=True,
            normalize_embeddings=True
        )
        
        df_chunks['embedding'] = [emb.tolist() for emb in embeddings]
        df_embeddings = df_chunks.copy()  # âœ… æ”¹ï¼šä¿ç•™æ‰€æœ‰åˆ—
    
    # âœ… æ”¹ï¼šé€‰æ‹©éœ€è¦çš„åˆ—
    df_embeddings = df_embeddings[['EventId', 'Sedol_6', 'StartDateEST', 'LastUpdateEST', 'chunk_seq_id', 'embedding']].copy()
    
  #  print(f"âœ… Generated {len(df_embeddings):,} embeddings")
   # print(f"   Embedding dimension: {len(df_embeddings['embedding'].iloc[0])}")
    
    return df_embeddings

# ============================================
# STEP 6: SAVE OUTPUTS
# ============================================

def save_outputs(meta: pd.DataFrame, embeddings: pd.DataFrame, T: str):
    """Save meta.parquet and embeddings.parquet"""
    import os
    
    # âœ… æ”¹ï¼šåˆ›å»ºå­ç›®å½•
    meta_dir = f"{OUTPUT_DIR}/meta"
    emb_dir = f"{OUTPUT_DIR}/embeddings"
    os.makedirs(meta_dir, exist_ok=True)
    os.makedirs(emb_dir, exist_ok=True)
    
    date_str = pd.to_datetime(T).strftime('%Y%m%d')
    
    # âœ… æ”¹ï¼šä¿å­˜åˆ°å­ç›®å½•ï¼Œæ–‡ä»¶ååªç”¨æ—¥æœŸ
    meta_path = f"{meta_dir}/{date_str}.parquet"
    meta.to_parquet(meta_path, index=False)
   # print(f"\nğŸ’¾ Saved meta: {meta_path}")
    #print(f"   Size: {len(meta):,} records")
    
    # Save embeddings
    if not embeddings.empty:
        emb_path = f"{emb_dir}/{date_str}.parquet"
        embeddings.to_parquet(emb_path, index=False)
      #  print(f"ğŸ’¾ Saved embeddings: {emb_path}")
      #  print(f"   Size: {len(embeddings):,} chunks")
    else:
        emb_path = None
       # print(f"â„¹ï¸ No embeddings to save")
    
    return meta_path, emb_path

# ============================================
# MAIN PIPELINE
# ============================================

def run_daily_snapshot(T: str):
    """
    ä¸»å‡½æ•°ï¼šè¿è¡Œå®Œæ•´çš„ daily snapshot pipeline
    
    Args:
        T: str, 'YYYY-MM-DD' - trading date (EOD)
        
    Returns:
        tuple: (meta_path, embeddings_path)
    """
    # print(f"\n{'#'*80}")
    # print(f"# LAYER 0 SNAPSHOT PIPELINE")
   # print(f"Date: {T}")
    #print(f"{'-'*80}")
    
    # Step 1: Fetch raw transcripts
    df_transcript = fetch_transcript_snapshot_for_date(T)
    
    # Step 2: Map to universe
    df_mapped = map_to_universe(df_transcript, T)
    if df_mapped.empty:
        print(f"\nâš ï¸ No universe data for {T}, skipping...")
        return None, None
    
    # Step 3: Build meta
    meta = build_meta_parquet(df_mapped, T)
    
    # å¦‚æœæ²¡æœ‰ä»»ä½• callsï¼Œç›´æ¥ä¿å­˜ meta å¹¶è¿”å›
    if df_mapped['EventId'].notna().sum() == 0:
        #print(f"\nâ„¹ï¸ No calls for {T}, saving meta only...")
        meta_path = save_outputs(meta, pd.DataFrame(), T)[0]
        return meta_path, None
    
    # Step 4: Clean transcripts (åªå¤„ç†æœ‰ call çš„)
   # print(f"\nğŸ§¹ Cleaning transcripts...")
    df_with_calls = df_mapped[df_mapped['Contents'].notna()].copy()
    df_result = df_with_calls['Contents'].apply(process_transcript_step0)
    df_with_calls = pd.concat([df_with_calls, df_result], axis=1)
    
    # Step 5: Detect QA
   # print(f"ğŸ” Detecting Q&A sections...")
    df_with_calls['has_qa'] = df_with_calls['contents_cleaned'].apply(detect_qa_section)
  #  print(f"   Has Q&A: {df_with_calls['has_qa'].sum()} / {len(df_with_calls)}")
    
    # Step 6: Split presentation and QA
  #  print(f"âœ‚ï¸ Splitting presentation and Q&A...")
    result = df_with_calls.apply(split_presentation_qa, axis=1)
    df_with_calls[['Presentation', 'QA']] = result
    
    # æ›´æ–° df_mapped
    df_mapped.loc[df_with_calls.index, ['contents_cleaned', 'has_qa', 'Presentation', 'QA']] = \
        df_with_calls[['contents_cleaned', 'has_qa', 'Presentation', 'QA']]
    
    # Step 7: Chunk presentations
    df_chunks = chunk_presentations(df_mapped)
    
    # Step 8: Generate embeddings
    if not df_chunks.empty:
        embeddings = generate_embeddings(df_chunks)
    else:
        embeddings = pd.DataFrame()
    
    # Step 9: Save outputs
    meta_path, emb_path = save_outputs(meta, embeddings, T)
    
    # print(f"\n{'='*80}")
    # print(f"âœ… PIPELINE COMPLETED FOR {T}")
    # print(f"{'='*80}")
    # print(f"Meta records: {len(meta):,}")
    # print(f"Embedding chunks: {len(embeddings):,}")
    
    return meta_path, emb_path


# ============================================
# BATCH PROCESSING
# ============================================

def run_batch_snapshots(start_date, end_date):
    """æ‰¹é‡è¿è¡Œå¤šä¸ªæ—¥æœŸ"""
    # ä½¿ç”¨ universe çš„å®é™…äº¤æ˜“æ—¥
    start_date = pd.to_datetime(start_date)
    end_date = pd.to_datetime(end_date)
    date_range = df_univ[(df_univ['date'] >= start_date) & 
                         (df_univ['date'] <= end_date)]['date'].unique()
    date_range = sorted(date_range)
    
    print(f"\n{'='*80}")
    print(f"BATCH PROCESSING")
    print(f"Date range: {start_date} to {end_date}")
    print(f"{'='*80}")
    
    results = []
    
    for date in date_range:
        date_str = pd.to_datetime(date).strftime('%Y-%m-%d')
        
        try:
            meta_path, emb_path = run_daily_snapshot(date_str)
            results.append({
                'date': date_str,
                'status': 'success' if meta_path else 'no_data',
                'meta_path': meta_path,
                'emb_path': emb_path
            })
        except Exception as e:
            print(f"\n Failed for {date_str}: {str(e)}")
            import traceback
            traceback.print_exc()
            results.append({
                'date': date_str,
                'status': 'failed',
                'error': str(e)
            })
            continue
    
    # Summary
    df_results = pd.DataFrame(results)
    # print(f"\n{'='*80}")
    # print(f"BATCH PROCESSING SUMMARY")
    # print(f"{'='*80}")
    # print(df_results['status'].value_counts())
    
    # Save results
    # results_path = f"{OUTPUT_DIR}/batch_results.csv"
    # df_results.to_csv(results_path, index=False)
   # print(f"\nğŸ’¾ Batch results saved to: {results_path}")
    
    return df_results


# ============================================
# USAGE
# ============================================

if __name__ == "__main__":
    
#    # Single date
#     T = '2024-01-17'
#     meta_path, emb_path = run_daily_snapshot(T)
    
  #  Batch processing
    # results = run_batch_snapshots(
    #     #start_date='2015-01-01',
    #     start_date='2025-12-29',
    #     end_date='2026-02-06'
    # )
    results = run_batch_snapshots(
        #start_date='2015-01-01',
        # start_date='2025-12-27',
        # end_date='2025-12-30'
        start_date='2026-02-10',
        end_date='2026-02-10'
    )




modelï¼š
import pandas as pd
import numpy as np
import random
import statsmodels.api as sm
import os
import matplotlib.pyplot as plt
import seaborn as sns
from pandas.tseries.offsets import BDay
import scipy.optimize as optimize
import pyodbc
from tqdm.notebook import tqdm
from scipy import stats

import warnings
from concurrent.futures import ThreadPoolExecutor, as_completed
from datetime import datetime
import time

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader
import torch.optim as optim
from tqdm import tqdm

# ============================================
# PRODUCTION MODEL TRAINING & TESTING
# ============================================

import pandas as pd
import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
from tqdm import tqdm
import random
from pathlib import Path
from sklearn.metrics import r2_score
from datetime import datetime
import warnings
warnings.filterwarnings('ignore')

# # ============================================
# # CONFIGURATION
# # ============================================

# ============================================
# STEP 1: LOAD AND MERGE META DATA
# ============================================
def load_universe(univ_dir):
    files = sorted(Path(univ_dir).glob("*.parquet"))
    # if not files:
    #     raise ValueError(f"No universe parquet files found in {univ_dir}")

    dfs = [pd.read_parquet(f) for f in files]
    univ = pd.concat(dfs, ignore_index=True)

    # if 'date' not in univ.columns:
    #     raise ValueError("Universe schema error: 'date' column not found")

    univ['date'] = pd.to_datetime(univ['date'])
    univ = univ.sort_values('date').reset_index(drop=True)
    return univ

def load_and_merge_meta(meta_dir, start_date, end_date):
    """
    åŠ è½½å¹¶åˆå¹¶æŒ‡å®šæ—¥æœŸèŒƒå›´å†…çš„æ‰€æœ‰ meta æ–‡ä»¶
    
    Args:
        meta_dir: meta æ–‡ä»¶ç›®å½•
        start_date: å¼€å§‹æ—¥æœŸ 'YYYY-MM-DD'
        end_date: ç»“æŸæ—¥æœŸ 'YYYY-MM-DD'
    
    Returns:
        åˆå¹¶åçš„ DataFrameï¼Œå·²å»é‡
    """
    print(f"\n{'='*80}")
    print(f"Loading meta data from {start_date} to {end_date}")
    print(f"{'='*80}")
    
    meta_files = sorted(Path(meta_dir).glob('*.parquet'))
    
    all_meta = []
    for file in tqdm(meta_files, desc="Loading meta files"):
        date_str = file.stem  
        file_date = pd.to_datetime(date_str, format='%Y%m%d')
        
        if pd.to_datetime(start_date) <= file_date <= pd.to_datetime(end_date):
            df = pd.read_parquet(file)
            all_meta.append(df)
    
    
    # åˆå¹¶æ‰€æœ‰ meta
    meta_combined = pd.concat(all_meta, ignore_index=True)
    
    # åªä¿ç•™æœ‰ call çš„è®°å½•
    meta_with_calls = meta_combined[meta_combined['EventId'].notna()].copy()
    
    # å»é‡ï¼šæŒ‰ (EventId, Sedol_6, StartDateEST, LastUpdateEST) å»é‡
    # ä¿ç•™æœ€æ—©çš„ dateï¼ˆcall é¦–æ¬¡å‡ºç°çš„æ—¥æœŸï¼‰
    #Deduplicating meta data
    original_len = len(meta_with_calls)
    
    meta_with_calls = (
        meta_with_calls
        .sort_values('date')  # æŒ‰ date æ’åº
        .drop_duplicates(
            subset=['EventId', 'Sedol_6', 'StartDateEST', 'LastUpdateEST'],
            keep='first'  # ä¿ç•™æœ€æ—©çš„ date
        )
    )
    
    print(f"After dedup: {len(meta_with_calls):,}")
    print(f"Removed: {original_len - len(meta_with_calls):,}")
    
    # è½¬æ¢æ—¥æœŸç±»å‹
    meta_with_calls['date'] = pd.to_datetime(meta_with_calls['date'])
    meta_with_calls['StartDateEST'] = pd.to_datetime(meta_with_calls['StartDateEST'])
    meta_with_calls['LastUpdateEST'] = pd.to_datetime(meta_with_calls['LastUpdateEST'])
    
    return meta_with_calls


# ============================================
# STEP 2: LOAD AND MERGE EMBEDDINGS
# ============================================

def load_and_merge_embeddings(embedding_dir, start_date, end_date, version='all'):
    """
    åŠ è½½å¹¶åˆå¹¶æŒ‡å®šæ—¥æœŸèŒƒå›´å†…çš„æ‰€æœ‰ embedding æ–‡ä»¶
    
    Returns:
        åˆå¹¶åçš„ DataFrameï¼Œå·²å»é‡å¹¶æŒ‰ call èšåˆ
    """
    print(f"\n{'='*80}")
    print(f"Loading embeddings from {start_date} to {end_date}")
    print(f"{'='*80}")
    
    emb_files = sorted(Path(embedding_dir).glob('*.parquet'))
    
    all_embeddings = []
    for file in tqdm(emb_files, desc="Loading embedding files"):
        date_str = file.stem
        file_date = pd.to_datetime(date_str, format='%Y%m%d')
        
        if pd.to_datetime(start_date) <= file_date <= pd.to_datetime(end_date):
            df = pd.read_parquet(file)
            all_embeddings.append(df)
    
    
    # åˆå¹¶æ‰€æœ‰ embeddings
    emb_combined = pd.concat(all_embeddings, ignore_index=True)
    
    # å»é‡ï¼šæŒ‰ (EventId, Sedol_6, StartDateEST, LastUpdateEST, chunk_seq_id) å»é‡,ä¸€ä¸ªcallå¯èƒ½æœ‰å¤šæ¡ï¼ŒåŒ¹é…åœ¨Tå’ŒT+1ï¼ˆå¦‚æœå‘ç”Ÿåœ¨0ç‚¹åˆ°9ï¼š30ä¹‹é—´ï¼‰
    #Deduplicating embeddings.
    original_len = len(emb_combined)
    
    emb_combined = emb_combined.drop_duplicates(
        subset=['EventId', 'Sedol_6', 'StartDateEST', 'LastUpdateEST', 'chunk_seq_id'],
        keep='first'
    )
    
    
    # æŒ‰ call èšåˆ chunks
    emb_combined['StartDateEST'] = pd.to_datetime(emb_combined['StartDateEST'])
    emb_combined['LastUpdateEST'] = pd.to_datetime(emb_combined['LastUpdateEST'])
    
    embedding_grouped = emb_combined.groupby(
        ['EventId', 'Sedol_6', 'StartDateEST', 'LastUpdateEST']
    ).apply(
        lambda x: x[['chunk_seq_id', 'embedding']].to_dict('records')
    ).reset_index(name='chunks')
    
    # print(f"Unique calls after grouping: {len(embedding_grouped):,}")
    
    # return embedding_grouped
      # âœ… æ–°å¢ï¼šæ ¹æ® version å‚æ•°é€‰æ‹©
    if version == 'first':
        print(f"Keeping FIRST update per call...")
        embedding_grouped = (
            embedding_grouped
            .sort_values('LastUpdateEST')
            .drop_duplicates(subset=['EventId', 'Sedol_6', 'StartDateEST'], keep='first')
        )
    elif version == 'last':
        print(f"Keeping LAST update per call...")
        embedding_grouped = (
            embedding_grouped
            .sort_values('LastUpdateEST')
            .drop_duplicates(subset=['EventId', 'Sedol_6', 'StartDateEST'], keep='last')
        )
    elif version == 'all':
        print(f"Keeping ALL updates per call...")
        # ä¸åšé¢å¤–å»é‡
        pass
    else:
        raise ValueError(f"Invalid version: {version}. Must be 'first', 'last', or 'all'")
    
    print(f"Unique calls after version selection: {len(embedding_grouped):,}")
    
    return embedding_grouped


# ============================================
# STEP 3: PREPARE UNIVERSE WITH FEATURES & TARGETS
# ============================================

def calculate_features_and_targets(df_univ):
    """
    è®¡ç®— features å’Œ targets
    """
    print(f"\nCalculating features and targets...")
    
    df_univ = df_univ.sort_values(['Sedol_6', 'date'])
    
    # Target: [T-1, T, T+1] mean return
    df_univ['target_ret'] = df_univ.groupby('Sedol_6', group_keys=False)['AdjSpecRet'].apply(
        lambda x: (x.shift(1) + x + x.shift(-1)) / 3
    )
    
    # Normalized target
    EPS = 1e-8
    df_univ['target_ret_norm'] = (
        df_univ['target_ret'] / (df_univ['SpecRisk'] + EPS)
    ).clip(lower=-5.0, upper=5.0)
    
    # Forward 60D return
    df_univ['fwd_60d_ret'] = df_univ.groupby('Sedol_6', group_keys=False)['AdjSpecRet'].transform(
        lambda x: x.shift(-2).rolling(window=60, min_periods=30).mean()
    )
    
    # Features
    # df_univ['EPS_ActualValue_Adj'] = (df_univ['IB0_QTR_EPS_ActualValue'] * df_univ['AdjFactor'])
    # df_univ['EPS_SurpriseMean_Adj'] = (df_univ['IB0_QTR_EPS_SurpriseMean'] * df_univ['AdjFactor'])
    # df_univ['EPS_Surprise'] = (df_univ['EPS_ActualValue_Adj'] - df_univ['EPS_SurpriseMean_Adj'])
    # df_univ['Revenue_Surprise'] = (df_univ['IB0_QTR_SAL_ActualValue'] - df_univ['IB0_QTR_SAL_SurpriseMean'])
    # df_univ['LogMktCap'] = np.log(df_univ['MktCap'] + 1)
    
    print(f"Finish features and targets calculation")
    
    return df_univ

def winsorize_features(df_univ, earnings_features, train_end, l_clip=0.0005, h_clip=0.9995):
    """
    åŸºäºè®­ç»ƒé›†è¿›è¡Œ winsorization
    """

    train_df = df_univ[df_univ['date'] <= train_end].copy()
    financial_features = []
    
    for col in earnings_features:
        # æ£€æŸ¥åˆ—æ˜¯å¦å­˜åœ¨
        if col not in df_univ.columns:
            print(f"warning: {col} not in universe, skipping")
            continue
        
        # è®¡ç®— clip thresholds
        lower = train_df[col].quantile(l_clip)
        upper = train_df[col].quantile(h_clip)
        
        # åˆ›å»º clipped åˆ—
        clipped_col = col + '_clipped'
        df_univ[clipped_col] = df_univ[col].clip(lower, upper)
        
        # å¡«å……ç¼ºå¤±å€¼ï¼ˆç”¨è®­ç»ƒé›†çš„ medianï¼‰
        median_val = train_df[col].median()
        df_univ[clipped_col] = df_univ[clipped_col].fillna(median_val)
        
        financial_features.append(clipped_col)
    print(f"\n{'='*80}")
    print(f"winsorization completed")
    print(f"\n{'='*80}")
    
    return df_univ, financial_features



def merge_data_for_training(meta_df, embedding_df, univ_df, financial_features, cut_hour=9, cut_minutes=30):
    """
    åˆå¹¶ meta, universe, embeddings ç”Ÿæˆè®­ç»ƒæ•°æ®
    """
    print(f"\n{'='*80}")
    print(f"Merging meta + universe + embeddings")
    print(f"{'='*80}")
    
    # Step 1: è®¡ç®— meta å¯¹åº”çš„ universe date
    print(f"Mapping StartDateEST to universe date...")
    print(f"  Meta records: {len(meta_df):,}")
    
    meta_df = meta_df.copy()
    meta_df['StartDateEST'] = pd.to_datetime(meta_df['StartDateEST'])
    
    # æå–æ—¥æœŸå’Œæ—¶é—´
    meta_df['call_date'] = meta_df['StartDateEST'].dt.normalize()
    meta_df['call_hour'] = meta_df['StartDateEST'].dt.hour
    meta_df['call_minute'] = meta_df['StartDateEST'].dt.minute
    
    # è·å–æ‰€æœ‰äº¤æ˜“æ—¥ï¼ˆä» universeï¼‰
    trading_dates = sorted(univ_df['date'].unique())
    #trading_dates_map = {date: i for i, date in enumerate(trading_dates)}
    
    def get_universe_date(row):
        """
        æ ¹æ® StartDateEST è®¡ç®—å¯¹åº”çš„ universe date
        - [00:00, 9:30): ä½¿ç”¨å‰ä¸€ä¸ªäº¤æ˜“æ—¥
        - [9:30, 24:00): ä½¿ç”¨å½“å¤©
        """
        call_date = row['call_date']
        call_hour = row['call_hour']
        call_minute = row['call_minute']
        
        # åˆ¤æ–­æ˜¯å¦åœ¨ 9:30 ä¹‹å‰
        if call_hour < cut_hour or (call_hour == cut_hour and call_minute < cut_minutes):
            prev_dates = [d for d in trading_dates if d < call_date]
            if prev_dates:
                return prev_dates[-1]
            else:
                return call_date - pd.offsets.BDay(1)
        else:
            # ä½¿ç”¨å½“å¤©
            return call_date
    
    meta_df['univ_date'] = meta_df.apply(get_universe_date, axis=1)
    
    # # ç»Ÿè®¡
    # before_16 = (meta_df['call_hour'] < 16).sum()
    # after_16 = (meta_df['call_hour'] >= 16).sum()
    # print(f"  Calls before 16:00: {before_16:,} (use same day)")
    # print(f"  Calls after 16:00: {after_16:,} (use next trading day)")
    
    # è¿‡æ»¤æ‰æ— æ³•åŒ¹é…çš„
    meta_df = meta_df[meta_df['univ_date'].notna()].copy()
    print(f"  After filtering: {len(meta_df):,} records")
    
    # Step 2: Meta + Universe
    print(f"\n Merging meta with universe...")
    print(f"  Universe records: {len(univ_df):,}")
    
    # âœ… ä¿®å¤ï¼šå…ˆé€‰æ‹©éœ€è¦çš„åˆ—ï¼Œé¿å…åˆ—åå†²çª
    meta_cols_to_keep = [
        'date', 'EventId', 'Sedol_6', 'PointID', 
        'StartDateEST', 'LastUpdateEST', 'univ_date'
    ]
    meta_for_merge = meta_df[meta_cols_to_keep].copy()

    # print(f"  Checking sample meta:")
    # print(meta_for_merge.head())
    # print(f"\n  Checking sample univ:")
    # print(univ_df[['date', 'PointID', 'Sedol_6']].head())

    # Merge: meta.univ_date = univ.date
    merged = meta_for_merge.merge(
        univ_df,
        left_on=['univ_date', 'PointID', 'Sedol_6'],
        right_on=['date', 'PointID', 'Sedol_6'],
        how='left',
        suffixes=('_meta', '_univ')  # âœ… æ·»åŠ åç¼€é¿å…å†²çª
    )
    #merged.to_parquet('merged.parquet')
    # print(f"\n  Checking sample merged:")
    # print(merged.head())
    # âœ… æ£€æŸ¥æœªåŒ¹é…çš„è®°å½•
    n_unmatched = merged['date_univ'].isna().sum()
    if n_unmatched > 0:
        print(f"\n WARNING: univ_df less than meta, {n_unmatched}/{len(merged)} records not matched with meta")

    # âœ… ä¿®å¤ï¼šé‡å‘½ååˆ—ï¼Œä¿æŒä¸€è‡´æ€§
    # merged = merged.rename(columns={
    #     'date_meta': 'meta_date',  # meta çš„ dateï¼ˆcall é¦–æ¬¡å‡ºç°æ—¥æœŸï¼‰(0~930å‰ä¸€å¤©ï¼Œ930åˆ°24å½“å¤©)
    #     'date_univ': 'date'        # universe çš„ dateï¼ˆç”¨äº featuresï¼‰
    # })
    merged = merged.rename(columns={
        'date_meta': 'date'#,  # meta çš„ dateï¼ˆcall é¦–æ¬¡å‡ºç°æ—¥æœŸï¼‰(0~930å‰ä¸€å¤©ï¼Œ930åˆ°24å½“å¤©)
       # 'date_univ': 'date_univ'        # universe çš„ dateï¼ˆç”¨äº featuresï¼‰
    })
    print(f"  After merge: {len(merged):,} records")
    
    if len(merged) == 0:
        print(f"\n WARNING: No records after merging meta with universe!")
        # print(f"  Checking sample meta dates:")
        # print(meta_df[['Sedol_6', 'StartDateEST', 'univ_date']].head())
        # print(f"\n  Checking sample universe dates:")
        # print(univ_df[['Sedol_6', 'date']].head())
        return pd.DataFrame()
    
    # Step 3: Add embeddings
    print(f"\n Merging with embeddings...")
    print(f"  Embedding calls: {len(embedding_df):,}")
    
    merged = merged.merge(
        embedding_df,
        on=['EventId', 'Sedol_6', 'StartDateEST', 'LastUpdateEST'],
        how='left'
    )
    n_unmatched2 = merged['chunks'].isna().sum()
    if n_unmatched2 > 0:
        print(f"\n WARNING: embedding less than meta, {n_unmatched2}/{len(merged)} records not matched with meta")

    print(f"  After merge: {len(merged):,} records")
    
    if len(merged) == 0:
        print(f"\n WARNING: No records after merging with embeddings!")
        return pd.DataFrame()
    
    # Step 4: Filter valid samples
    print(f"\n Filtering valid samples...")
    valid_mask = (
        merged['target_ret_norm'].notna() &
        merged['chunks'].notna()
    )
    
    merged_valid = merged[valid_mask].copy()
    
    print(f"  Valid samples: {len(merged_valid):,}")
  #  print(f"  Unique calls: {merged_valid['EventId'].nunique():,}")
    
    if len(merged_valid) == 0:
        print(f"\n WARNING: No valid samples!")
        return pd.DataFrame()
    
    # Step 5: Select required columns
    required_cols = (
        ['date', 'EventId', 'Sedol_6', 'PointID', 
         'StartDateEST', 'LastUpdateEST', 'date_univ'] +
        financial_features +
        ['target_ret_norm', 'fwd_60d_ret', 'chunks']
    )
    
    merged_valid = merged_valid[required_cols].copy()
    
    return merged_valid

# ============================================
# STEP 5: DATASET & DATALOADER
# ============================================

class CallEarningsDataset(Dataset):
    def __init__(self, df, financial_features):
        self.df = df.reset_index(drop=True)
        self.financial_features = financial_features
        self.features_array = self.df[financial_features].to_numpy(dtype=np.float32)
        self.targets_array = self.df['target_ret_norm'].to_numpy(dtype=np.float32)
        
    def __len__(self):
        return len(self.df)
    
    def __getitem__(self, idx):
        row = self.df.iloc[idx]
        chunks = row['chunks']
        embeddings = [np.array(c['embedding'], dtype=np.float32) for c in chunks]
        embeddings = torch.from_numpy(np.stack(embeddings))
        features = torch.from_numpy(self.features_array[idx])
        target = torch.tensor(self.targets_array[idx], dtype=torch.float32)
        
        return {
            'embeddings': embeddings,
            'features': features,
            'target': target,
            'fwd_60d_ret': row['fwd_60d_ret'],
            'n_chunks': len(embeddings),
            'date': row['date'],
            'Sedol_6': row['Sedol_6'],
            'PointID': row['PointID'],
            'EventId': row['EventId'],
            'StartDateEST': row['StartDateEST'],
            'LastUpdateEST': row['LastUpdateEST']
        }


def collate_fn(batch):
    max_chunks = max([item['n_chunks'] for item in batch])
    embeddings_padded = []
    masks = []
    features = []
    targets = []
    fwd_60d_ret = []
    dates = []
    Sedol_6 = []
    PointID = []
    EventId = []
    StartDateEST = []
    LastUpdateEST = []

    for item in batch:
        emb = item['embeddings']
        N = emb.shape[0]
        
        if N < max_chunks:
            padding = torch.zeros(max_chunks - N, 768)
            emb_padded = torch.cat([emb, padding], dim=0)
            mask = torch.cat([torch.ones(N), torch.zeros(max_chunks - N)])
        else:
            emb_padded = emb
            mask = torch.ones(N)
        
        embeddings_padded.append(emb_padded)
        masks.append(mask)
        features.append(item['features'])
        targets.append(item['target'])
        fwd_60d_ret.append(item['fwd_60d_ret'])
        dates.append(item['date'])
        Sedol_6.append(item['Sedol_6'])
        PointID.append(item['PointID'])
        EventId.append(item['EventId'])
        StartDateEST.append(item['StartDateEST'])
        LastUpdateEST.append(item['LastUpdateEST'])
    
    return {
        'embeddings': torch.stack(embeddings_padded),
        'masks': torch.stack(masks),
        'features': torch.stack(features),
        'targets': torch.stack(targets),
        'fwd_60d_ret': torch.tensor(fwd_60d_ret, dtype=torch.float32),
        'dates': dates,
        'Sedol_6': Sedol_6,
        'PointID': PointID,
        'EventId': EventId,
        'StartDateEST': StartDateEST,
        'LastUpdateEST': LastUpdateEST
    }


# ============================================
# STEP 6: MODEL
# ============================================

class TextConditionalLinearModel(nn.Module):
    def __init__(self, embed_dim=768, coeff_hidden_dim=128, attn_hidden_dim=256, coeff_dropout=0.2):
        super().__init__()
        self.embed_dim = embed_dim
        
        self.attention = nn.Sequential(
            nn.Linear(embed_dim, attn_hidden_dim),
            nn.Tanh(),
            nn.Linear(attn_hidden_dim, 1)
        )
        
        self.coeff_net = nn.Sequential(
            nn.Linear(embed_dim, coeff_hidden_dim),
            nn.ReLU(),
            nn.Dropout(coeff_dropout),
            nn.Linear(coeff_hidden_dim, 1)
        )
        
        nn.init.xavier_uniform_(self.coeff_net[-1].weight, gain=0.01)
        nn.init.zeros_(self.coeff_net[-1].bias)
        
    def forward(self, embeddings, masks):
        B, N, D = embeddings.shape
        
        attn_scores = self.attention(embeddings).squeeze(-1)
        attn_scores = attn_scores.masked_fill(masks == 0, -1e9)
        attn_weights = F.softmax(attn_scores, dim=1)
        
        z_call = torch.bmm(
            attn_weights.unsqueeze(1),
            embeddings
        ).squeeze(1)
        
        predictions = self.coeff_net(z_call).squeeze(-1)
        
        return predictions, attn_weights, z_call


# ============================================
# STEP 7: TRAINING & EVALUATION
# ============================================

def set_seed(seed):
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False


def train_epoch(model, loader, optimizer, device):
    model.train()
    total_loss = 0
    
    pbar = tqdm(loader, desc='Training', leave=False)
    for batch in pbar:
        embeddings = batch['embeddings'].to(device)
        masks = batch['masks'].to(device)
        targets = batch['targets'].to(device)
        
        predictions, _, _ = model(embeddings, masks)
        loss = F.mse_loss(predictions, targets)
        
        optimizer.zero_grad()
        loss.backward()
        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
        optimizer.step()
        
        total_loss += loss.item()
        pbar.set_postfix({'loss': f'{loss.item():.6f}'})
    
    return {'loss': total_loss / len(loader)}


def evaluate_with_daily_ic(model, loader, device, eval_version='all'):
    model.eval()
    
    all_preds = []
    all_targets = []
    all_fwd_60d_ret = []
    all_dates = []
    all_sedols = []
    all_PointIDs = []
    all_EventIds = []
    all_StartDateEST = []
    all_LastUpdateEST = []
    all_attn_weights = []
    all_z_calls = []
    
    with torch.no_grad():
        for batch in tqdm(loader, desc='Evaluating', leave=False):
            embeddings = batch['embeddings'].to(device)
            masks = batch['masks'].to(device)
            targets = batch['targets'].to(device)
            
            predictions, attn_weights, z_call = model(embeddings, masks)
            
            all_preds.extend(predictions.cpu().numpy())
            all_targets.extend(targets.cpu().numpy())
            all_fwd_60d_ret.extend(batch['fwd_60d_ret'].cpu().numpy())
            all_dates.extend(batch['dates'])
            all_sedols.extend(batch['Sedol_6'])
            all_PointIDs.extend(batch['PointID'])
            all_EventIds.extend(batch['EventId'])
            all_StartDateEST.extend(batch['StartDateEST'])
            all_LastUpdateEST.extend(batch['LastUpdateEST'])
            all_attn_weights.extend(attn_weights.cpu().numpy())
            all_z_calls.extend(z_call.cpu().numpy())

    all_preds = np.array(all_preds)
    all_targets = np.array(all_targets)
    

    
    results_df = pd.DataFrame({
        'pred': all_preds,
        'target': all_targets,
        'fwd_60d_ret': all_fwd_60d_ret,
        'pred_date': pd.to_datetime(all_dates),
        'Sedol_6': all_sedols,
        'PointID': all_PointIDs,
        'EventId': all_EventIds,
        'StartDateEST': all_StartDateEST,
        'LastUpdateEST': all_LastUpdateEST,
        'attn_weights': all_attn_weights,
        'aggregated_embeddings': all_z_calls,
    })

    print(f"\nEvaluation version strategy: {eval_version}")
    original_len = len(results_df)
    
    if eval_version == 'first':
        results_df = (
            results_df
            .sort_values('LastUpdateEST')
            .drop_duplicates(subset=['EventId', 'Sedol_6', 'StartDateEST'], keep='first')
        )
    elif eval_version == 'last':
        results_df = (
            results_df
            .sort_values('LastUpdateEST')
            .drop_duplicates(subset=['EventId', 'Sedol_6', 'StartDateEST'], keep='last')
        )
    elif eval_version == 'all':
        # ä½¿ç”¨æ‰€æœ‰ç‰ˆæœ¬
        pass
    else:
        raise ValueError(f"Invalid eval_version: {eval_version}")

    mse = np.mean((all_preds - all_targets) ** 2)
    rmse = np.sqrt(mse)
    
    # Daily IC
    daily_ics = results_df.groupby('pred_date', group_keys=False).apply(
        lambda x: x['pred'].corr(x['target'], method='spearman') if len(x) >= 10 else np.nan
    ).dropna()

    mean_ic = daily_ics.mean()
    ic_std = daily_ics.std()
    ic_ir = mean_ic / ic_std if ic_std > 0 else 0
    
    # Global correlations
    global_pearson = results_df['pred'].corr(results_df['target'], method='pearson')
    global_spearman = results_df['pred'].corr(results_df['target'], method='spearman')
    r_squared = r2_score(results_df['target'], results_df['pred'])
    
    # Forward 60D IC
    valid_fwd = results_df[results_df['fwd_60d_ret'].notna()].copy()
    daily_ics_fwd = valid_fwd.groupby('pred_date', group_keys=False).apply(
        lambda x: x['pred'].corr(x['fwd_60d_ret'], method='spearman') if len(x) >= 10 else np.nan
    ).dropna()

    mean_ic_fwd = daily_ics_fwd.mean()
    global_spearman_fwd = valid_fwd['pred'].corr(valid_fwd['fwd_60d_ret'], method='spearman')
    
    return {
        'mse': mse,
        'rmse': rmse,
        'mean_daily_ic': mean_ic,
        'ic_std': ic_std,
        'ic_ir': ic_ir,
        'daily_ics': daily_ics,
        'global_pearson': global_pearson,
        'global_spearman': global_spearman,
        'r_squared': r_squared,
        'mean_daily_ic_60D': mean_ic_fwd,
        'global_spearman_60D': global_spearman_fwd,
        'results_df': results_df
    }



# ============================================
# MAIN PIPELINE
# ============================================

def main(config):
    META_DIR = config['meta_dir']
    EMBEDDING_DIR = config['embedding_dir']
    UNIVERSE_PATH = config['universe_path']
    OUTPUT_DIR = config['output_dir']
    
    TRAIN_START = config['train_start']
    TRAIN_END = config['train_end']
    TEST_START = config['test_start']
    TEST_END = config['test_end']
    
    SEED = config['seed']
    LEARNING_RATE = config['learning_rate']
    N_EPOCHS = config['n_epochs']
    BATCH_SIZE = config['batch_size']
    
    UPDATE_VERSION_TRAIN = config['update_version_train']
    UPDATE_VERSION_EVAL = config['update_version_eval']
    
    earnings_features = config['earnings_features']
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    Path(OUTPUT_DIR).mkdir(parents=True, exist_ok=True)

    print(f"\n{'#'*80}")
    print(f"# PRODUCTION MODEL TRAINING & TESTING")
    print(f"# Seed: {SEED}")
    print(f"{'#'*80}\n")
    
    set_seed(SEED)
    
    # Step 1: Load meta data
    train_meta = load_and_merge_meta(META_DIR, TRAIN_START, TRAIN_END)
    test_meta = load_and_merge_meta(META_DIR, TEST_START, TEST_END)
    
    # Step 2: Load embeddings
    train_embeddings = load_and_merge_embeddings(EMBEDDING_DIR, TRAIN_START, TRAIN_END, version=UPDATE_VERSION_TRAIN)
    test_embeddings = load_and_merge_embeddings(EMBEDDING_DIR, TEST_START, TEST_END, version='all')
    
    # Step 3: Prepare universe (åŠ è½½å®Œæ•´çš„ universeï¼Œä¸è¦è¿‡æ»¤æ—¥æœŸ)
    print(f"\n{'='*80}")
    print(f"Loading universe data")
    print(f"{'='*80}")
    
    # univ_df = pd.read_parquet(UNIVERSE_PATH)
    # univ_df['date'] = pd.to_datetime(univ_df['date'])
    univ_df = load_universe(UNIVERSE_PATH)
    univ_start = pd.to_datetime(TRAIN_START) - pd.offsets.BDay(1)
    univ_df = univ_df[(univ_df['date'] >= univ_start)].copy()
    
    # âœ… ä¿®æ”¹ï¼šä¸è¦åœ¨è¿™é‡Œè¿‡æ»¤æ—¥æœŸï¼Œå› ä¸ºéœ€è¦å®Œæ•´çš„äº¤æ˜“æ—¥å†
    print(f"Universe records: {len(univ_df):,}")
    print(f"Date range: {univ_df['date'].min()} to {univ_df['date'].max()}")
    
    # Sedol_6
    univ_df['Sedol_6'] = (
        univ_df['Sedol'].astype(str).str.strip().str.upper().str[:6]
    )
    
    # è®¡ç®— features å’Œ targets
    univ_df = calculate_features_and_targets(univ_df)
    
    # Step 4: Winsorize features
    univ_df, financial_features = winsorize_features(univ_df, earnings_features, TRAIN_END)
    
    # Step 5: Merge data
    print(f"\n{'='*80}")
    print(f"Preparing training data")
    print(f"{'='*80}")
    
    train_df = merge_data_for_training(
        train_meta,
        train_embeddings,
        univ_df,  
        financial_features
    )
    
    print(f"\n{'='*80}")
    print(f"Preparing testing data")
    print(f"{'='*80}")
    
    test_df = merge_data_for_training(
        test_meta,
        test_embeddings,
        univ_df,  
        financial_features
    )
    
    # âœ… æ·»åŠ ï¼šæ£€æŸ¥æ˜¯å¦æœ‰æ•°æ®
    if len(train_df) == 0:
        raise ValueError("Training data is empty! Check merge logic.")
    
    if len(test_df) == 0:
        raise ValueError("Testing data is empty! Check merge logic.")
    
    print(f"\n{'='*80}")
    print(f"  Training: {len(train_df):,} samples")
    print(f"  Testing: {len(test_df):,} samples")
    print(f"{'='*80}\n")
    
    # Step 6: Create datasets
    train_dataset = CallEarningsDataset(train_df, financial_features)
    test_dataset = CallEarningsDataset(test_df, financial_features)
    
    # train_loader = DataLoader(
    #     train_dataset,
    #     batch_size=BATCH_SIZE,
    #     shuffle=True,
    #     collate_fn=collate_fn,
    #     num_workers=0,
    #     generator=torch.Generator().manual_seed(SEED)
    # )
    
    test_loader = DataLoader(
        test_dataset,
        batch_size=64,
        shuffle=False,
        collate_fn=collate_fn,
        num_workers=0
    )
    
    # Step 7: Train model
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"Using device: {device}\n")
    
    model = TextConditionalLinearModel(
        embed_dim=768,
        coeff_hidden_dim=128,
        attn_hidden_dim=256,
        coeff_dropout=0.2
    ).to(device)
    
    optimizer = optim.AdamW(
        model.parameters(),
        lr=LEARNING_RATE,
        weight_decay=1e-3
    )
    
    print(f"{'='*80}")
    print(f"Training model...")
    print(f"{'='*80}\n")
    
    experiment_id = 0
    for epoch in range(N_EPOCHS):
        experiment_id += 1
        print(f"Epoch {epoch+1}/{N_EPOCHS}")
        train_loader = DataLoader(
        train_dataset,
        batch_size=BATCH_SIZE,
        shuffle=True,
        collate_fn=collate_fn,
        num_workers=0,
        generator=torch.Generator().manual_seed(SEED + experiment_id)
        )
        train_metrics = train_epoch(model, train_loader, optimizer, device)
        print(f"Train Loss: {train_metrics['loss']:.6f}\n")
    
    # Step 8: Evaluate
    print(f"{'='*80}")
    print(f"Evaluating on test set...")
    print(f"{'='*80}\n")
    
    test_metrics = evaluate_with_daily_ic(model, test_loader, device, eval_version=UPDATE_VERSION_EVAL)
    
    # Step 9: Save results
    results_df = test_metrics['results_df']
    output_path = f"{OUTPUT_DIR}/production_results_train{UPDATE_VERSION_TRAIN}_eval{UPDATE_VERSION_EVAL}_{N_EPOCHS}_{LEARNING_RATE}.parquet"
    results_df.to_parquet(output_path, index=False)
    
    print(f"\n{'='*80}")
    print(f"Test Set Performance")
    print(f"{'='*80}")
    print(f"MSE:                    {test_metrics['mse']:.6f}")
    print(f"RMSE:                   {test_metrics['rmse']:.6f}")
    print(f"Mean Daily IC:          {test_metrics['mean_daily_ic']:.6f}")
    print(f"Daily IC Std:           {test_metrics['ic_std']:.6f}")
    print(f"Daily IC IR:            {test_metrics['ic_ir']:.6f}")
    print(f"Global Pearson:         {test_metrics['global_pearson']:.6f}")
    print(f"Global Spearman:        {test_metrics['global_spearman']:.6f}")
    print(f"R Squared:              {test_metrics['r_squared']:.6f}")
    print(f"Mean Daily IC (60D):    {test_metrics['mean_daily_ic_60D']:.6f}")
    print(f"Global Spearman (60D):  {test_metrics['global_spearman_60D']:.6f}")
    print(f"{'='*80}\n")
    
    print(f"âœ… Results saved to: {output_path}")
    
    # Save model
    # model_path = f"{OUTPUT_DIR}/production_model_{SEED}.pth"
    # torch.save(model.state_dict(), model_path)
    # print(f"âœ… Model saved to: {model_path}")


#last all
config = {
    # Paths
    'meta_dir': '/data09/home/aw_shared/prod/daily_meta_embedding/meta',
    'embedding_dir': '/data09/home/aw_shared/prod/daily_meta_embedding/embeddings',
    'universe_path': '/data09/home/aw_shared/prod/univ_merged',
    #'/data09/home/aw_shared/prod/univ_merged/univ_merged.parquet',
    'output_dir': '/data09/home/aw_shared/prod/model_results',
    
    #Training parameters
    'train_start': '2015-01-01',
    'train_end': '2022-12-31',
    'test_start': '2023-01-01',
    'test_end': '2026-02-06', #'2025-12-29',
    # 'train_start': '2015-01-01',
    # 'train_end': '2016-12-31',
    # 'test_start': '2023-01-01',
    # 'test_end': '2025-12-29',
    
    # Model parameters
    'seed': 42,
    'learning_rate': 0.00001,#0.0001,
    'n_epochs': 2,#6,
    'batch_size': 32,
    
    # Version control
    'update_version_train': 'last',  # 'first', 'last', 'all'
    'update_version_eval': 'all',   # 'first', 'last', 'all'
    
    # Features
    'earnings_features': [
        # 'EPS_Surprise', 'Revenue_Surprise', 'EPS_SurpriseMean_Adj',
        # 'IB0_QTR_SAL_SurpriseMean', 'YOY_OEPS_G', 'YOY_SAL_G',
        # 'YOY_CPS_G', 'YOY_EPS_EG', 'YOY_SAL_EG', 'IB0_QTR1_EPS_Mean',
        # 'LogMktCap',
          'DlyReturn', 'AdjSpecRet'
    ] 
}

main(config)

Copilot Search Branding




Global web icon
TIME.IS
https://time.is â€º zh â€º United_States
Translate this result
ç°åœ¨çš„ç¾å›½æ—¶é—´ - Time.is
ç¾å›½çš„æ—¶é—´ã€æ—¶åŒºã€æ—¶å·®ã€æ—¥å‡ºæ—¥è½æ—¶é—´ç­‰ä¿¡æ¯ã€‚
Capital
Washington, D.C.
Largest city
New York City
Official languages
English
Government
Federal presidential republic
National September 11 Memorial & Museum
Grand Canyon National Park
Walt Disney World


ä¸–ç•Œæ—¶é’Ÿ :: ç¾å›½ (United States) - å½“å‰æ—¶é—´
ä¸–ç•Œæ—¶é—´ - ç¾å›½ (United States) - ç°åœ¨ä¸–ç•Œå„åœ°æ˜¯ä»€ä¹ˆæ—¶é—´ï¼Ÿ

 


ç¾å›½æ—¶é—´ç½‘æä¾›åœ¨çº¿æŸ¥è¯¢ç¾å›½æ—¶é—´ç°åœ¨å‡ æœˆå‡ æ—¥å‡ ç‚¹å‡ åˆ†å‡ ç§’ï¼Œæ¢ç®—ç¾å›½æ—¶é—´ä¸æœ¬åœ°æ—¶é—´ç›¸å·®å¤šå°‘ï¼Œç¾å›½æ—¶é—´ä¸æœ¬åœ°æ—¶é—´å·®ï¼Œè¿˜å¯è¿›è¡Œç¾å›½æ—¶é—´åœ¨çº¿æ ¡å‡†ï¼Œæ›´æœ‰ç¾å›½çš„æ—¶åŒºã€æ ‡å‡†æ—¶é—´ã€æ—¶å·®å¯¹ç…§è¡¨ç­‰ä¿¡æ¯ã€‚

åŒ—äº¬æ—¶é—´ç½‘
ç¾å›½æ—¶é—´ç°åœ¨å‡ æœˆå‡ æ—¥å‡ ç‚¹å‡ åˆ†å‡ ç§’åœ¨çº¿æ˜¾ç¤ºã€æŸ¥è¯¢_ç¾å›½æ—¶é—´ç½‘
 

Global web icon
World Clock
https://24timezones.com â€º ç¾å›½ â€º æ—¶é—´
Translate this result
ä¸–ç•Œæ—¶é’Ÿ :: ç¾å›½ (United States) - å½“å‰æ—¶é—´
ä¸–ç•Œæ—¶é—´ - ç¾å›½ (United States) - ç°åœ¨ä¸–ç•Œå„åœ°æ˜¯ä»€ä¹ˆæ—¶é—´ï¼Ÿ

 

Global web icon
true-time.com
https://zh.true-time.com â€º us
Translate this result
ç¾å›½ å½“å‰æ—¶é—´
ç¾å›½ å®æ—¶ç²¾ç¡®æ—¶é—´ï¼ˆå«ç§’ï¼‰ã€‚ åŒ…æ‹¬ ç¾å›½ çš„å¤ä»¤æ—¶è½¬æ¢ã€‚ å½“å‰æ—¶åŒº UTC-05:00. ä¿¡æ¯: é™†åœ°åŒºåŸŸï¼ˆæ´²ï¼‰, å›½æ——, é¦–éƒ½, é¢ç§¯, äººå£, è´§å¸, å›½å®¶ç”µè¯åŒºå·, äº¤é€šè¡Œé©¶æ–¹å‘.


Global web icon
TimeAndDate
https://www.timeanddate.com â€º worldclock â€º usa
Time in the United States
United States time now. USA time zones and time zone map with current time in each state.

Global web icon
time163.com
https://a.time163.com
Translate this result
ç¾å›½æ—¶é—´ç°åœ¨å‡ æœˆå‡ æ—¥å‡ ç‚¹å‡ åˆ†å‡ ç§’åœ¨çº¿æ˜¾ç¤ºã€æŸ¥è¯¢_ç¾å›½æ—¶é—´ç½‘
2 days ago Â· ç¾å›½æ—¶é—´ç½‘æä¾›åœ¨çº¿æŸ¥è¯¢ç¾å›½æ—¶é—´ç°åœ¨å‡ æœˆå‡ æ—¥å‡ ç‚¹å‡ åˆ†å‡ ç§’ï¼Œæ¢ç®—ç¾å›½æ—¶é—´ä¸æœ¬åœ°æ—¶é—´ç›¸å·®å¤šå°‘ï¼Œç¾å›½æ—¶é—´ä¸æœ¬åœ°æ—¶é—´å·®ï¼Œè¿˜å¯è¿›è¡Œç¾å›½æ—¶é—´åœ¨çº¿æ ¡å‡†ï¼Œæ›´æœ‰ç¾å›½çš„æ—¶åŒºã€æ ‡å‡†æ—¶é—´ã€æ—¶å·®å¯¹ â€¦


Global web icon
æ—¶é—´æŸ¥è¯¢ç½‘
https://www.timecha.com â€º time â€º america
Translate this result
ç¾å›½æ—¶é—´_ç¾å›½æ—¶é—´ç°åœ¨å‡ ç‚¹_ç¾å›½ä¸åŒ—äº¬æ—¶å·®
æ—¶é—´æŸ¥è¯¢ä¸ºæ‚¨å…è´¹æä¾›åœ¨çº¿æŸ¥è¯¢ç¾å›½æ—¶é—´ç°åœ¨å‡ ç‚¹é’Ÿï¼Œæ¢ç®—ç¾å›½æ—¶é—´ä¸åŒ—äº¬æ—¶é—´ç›¸å·®å¤šå°‘ï¼Œç¾å›½æ—¶é—´ä¸ä¸­å›½æ—¶é—´å·®ï¼Œè¿˜å¯è¿›è¡Œç¾å›½æ—¶é—´åœ¨çº¿æ ¡å‡†ï¼Œæ›´æœ‰ç¾å›½çš„æ—¶åŒºã€ â€¦

Global web icon
aichaxun.com
https://shijian.aichaxun.com â€º UnitedStates
Translate this res
